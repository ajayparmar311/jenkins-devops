import json
import logging
import threading
import time
import re
from typing import Optional

import pika
import requests
from fastapi import FastAPI, Request, HTTPException, Response
import uvicorn
from pydantic import BaseModel

# OTLP Exporter
from opentelemetry import metrics
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter

# =========================
# CONFIG
# =========================
RABBITMQ_HOST = "rabbitmq"
RABBITMQ_PORT = 5672
LOG_QUEUE = "logs_queue"
METRIC_QUEUE = "otel-metrics"
DOWNSTREAM_URL = "http://post-flask-app:5001/v1/metrics"
store_id = "5555"

OTEL_EXPORTER_ENDPOINT = "http://otel-collector:4318/v1/metrics"

# =========================
# LOGGING SETUP
# =========================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

# =========================
# FASTAPI APP
# =========================
app = FastAPI()

# =========================
# OTLP METRIC EXPORTER
# =========================
otlp_exporter = OTLPMetricExporter(
    endpoint=OTEL_EXPORTER_ENDPOINT
)

reader = PeriodicExportingMetricReader(otlp_exporter, export_interval_millis=5000)
provider = MeterProvider(metric_readers=[reader])
metrics.set_meter_provider(provider)

meter = metrics.get_meter("log-metric-producer")

# Counter for error logs
ERROR_COUNTER = meter.create_counter(
    name="app_error_logs_total",
    description="Total error logs observed from /log endpoint",
    unit="1"
)

# =========================
# RABBITMQ UTILS
# =========================
def get_connection():
    while True:
        try:
            connection = pika.BlockingConnection(pika.ConnectionParameters(host=RABBITMQ_HOST))
            channel = connection.channel()
            channel.queue_declare(queue=LOG_QUEUE, durable=True)
            channel.queue_declare(queue=METRIC_QUEUE, durable=True)
            return connection, channel
        except Exception as e:
            logging.error(f"Failed to connect to RabbitMQ: {e}, retrying in 5s...")
            time.sleep(5)

def get_rabbitmq_channel():
    connection, channel = get_connection()
    return connection, channel

# =========================
# METRIC RECORDER
# =========================
def record_metrics(message: dict):
    """Extract and push metrics to OTLP exporter"""
    if message.get("message_id") == "LOG_ERROR":
        match = re.search(r":\s*(\d+)", message.get("event_value", ""))
        cam_id = match.group(1) if match else "unknown"

        labels = {
            "app_name": message.get("app_info", "unknown"),
            "store": "1111",
            "filter_type": message.get("message_id"),
            "error_type": message.get("event", "UNKNOWN"),
            "cam_id": cam_id
        }

        ERROR_COUNTER.add(1, labels)
        logging.info(f"üìä Sent OTLP metric for error log: {labels}")

# =========================
# DOWNSTREAM / CALLBACK / CONSUMERS
# =========================
def send_downstream(body, is_metric=False):
    try:
        payload = json.loads(body.decode("utf-8"))
    except Exception as e:
        logging.error(f"‚ö†Ô∏è Could not decode message body: {e}")
        return False

    while True:
        try:
            res = requests.post(DOWNSTREAM_URL, json=payload, timeout=5)
            res.raise_for_status()
            logging.info(f"‚úÖ Sent to downstream: {payload}")
            return True
        except Exception as e:
            logging.error(f"üåê Downstream error: {e}, retrying in 5s...")
            time.sleep(5)

def callback(ch, method, properties, body, queue_type="logs"):
    logging.info(f"üì• Got message from {queue_type}: {body}...")
    ok = send_downstream(body, is_metric=(queue_type == "metrics"))
    if ok:
        ch.basic_ack(delivery_tag=method.delivery_tag)
    else:
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)

def start_consumer(queue_name, queue_type="logs"):
    while True:
        try:
            connection, channel = get_connection()
            channel.basic_qos(prefetch_count=1)
            channel.basic_consume(
                queue=queue_name,
                on_message_callback=lambda ch, method, props, body: callback(ch, method, props, body, queue_type)
            )
            logging.info(f"üöÄ RabbitMQ consumer started for {queue_type} queue: {queue_name}")
            channel.start_consuming()
        except Exception as e:
            logging.error(f"‚ùå Consumer for {queue_type} crashed: {e}, retrying in 5s...")
            time.sleep(5)

# =========================
# REQUEST MODEL
# =========================
class LogData(BaseModel):
    app_info: str
    message_id: str
    event: str
    event_value: str
    timestamp: Optional[str] = None
    duration_ms: Optional[int] = None
    user_id: Optional[str] = None

# =========================
# /log ENDPOINT
# =========================
@app.post("/log")
async def log_message(request: Request):
    try:
        data = await request.json()
        record_metrics(data)  # Push metrics via OTLP exporter

        timestamp = data.get("timestamp") or time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
        payload = {
            "store_id": "store_123",
            "timestamp": timestamp,
            "app_info": data.get("app_info"),
            "message_id": data.get("message_id"),
            "event": data.get("event"),
            "event_value": data.get("event_value"),
            "insert_id": f"unique_message_id_{store_id}_{int(time.time())}"
        }

        connection, channel = get_rabbitmq_channel()
        channel.basic_publish(
            exchange="",
            routing_key=LOG_QUEUE,
            body=json.dumps(payload),
            properties=pika.BasicProperties(delivery_mode=2)
        )
        connection.close()
        logging.info(f"üì§ Published to RabbitMQ logs_queue: {payload}")
        return {"status": "Message sent to RabbitMQ", "data": data}
    except Exception as e:
        logging.error(f"Error processing log: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =========================
# STARTUP
# =========================
@app.on_event("startup")
def startup_event():
    logging.info("‚ö° Launching RabbitMQ consumers in background threads...")
    threading.Thread(target=start_consumer, args=(LOG_QUEUE, "logs"), daemon=True).start()
    threading.Thread(target=start_consumer, args=(METRIC_QUEUE, "metrics"), daemon=True).start()

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5000)
