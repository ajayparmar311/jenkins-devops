import json
import logging
import threading
import time

import pika
import requests
from fastapi import FastAPI, Request, HTTPException
import uvicorn

# =========================
# CONFIG
# =========================
RABBITMQ_HOST = "rabbitmq"
RABBITMQ_PORT = 5672
LOG_QUEUE = "logs_queue"
METRIC_QUEUE = "otel-metrics"
DOWNSTREAM_URL = "http://post-flask-app:5001/v1/metrics"
store_id = "5555"

# =========================
# LOGGING SETUP
# =========================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

# =========================
# FASTAPI APP
# =========================
app = FastAPI()

# ---------- Helpers ----------
def safe_json_str(val) -> str:
    """Return a JSON string regardless of input type (dict/str/bytes/None)."""
    try:
        if isinstance(val, bytes):
            val = val.decode("utf-8", errors="replace")
        if isinstance(val, dict):
            return json.dumps(val, ensure_ascii=False)
        if isinstance(val, str):
            # verify it is valid JSON; if not, wrap it
            try:
                json.loads(val)
                return val
            except Exception:
                return json.dumps({"value": val}, ensure_ascii=False)
        if val is None:
            return "{}"
        # fallback: stringify
        return json.dumps(val, ensure_ascii=False)
    except Exception:
        return "{}"

def decode_body_to_obj(body):
    """Decode bytes -> JSON obj; pass through dict/list."""
    if isinstance(body, (dict, list)):
        return body
    if isinstance(body, bytes):
        try:
            return json.loads(body.decode("utf-8"))
        except Exception as e:
            logging.error(f"‚ùå Failed to decode JSON bytes: {e}")
            return None
    if isinstance(body, str):
        try:
            return json.loads(body)
        except Exception as e:
            logging.error(f"‚ùå Failed to decode JSON string: {e}")
            return None
    logging.error("‚ùå Unsupported body type for JSON decode")
    return None

def _attrs_list_to_dict(attrs_list):
    """
    OTLP attributes are like:
      [{"key": "cpu", "value": {"stringValue": "cpu0"}}, ...]
    This flattens to {"cpu": "cpu0", ...}
    """
    out = {}
    for a in attrs_list or []:
        k = a.get("key")
        v = a.get("value", {})
        if isinstance(v, dict) and v:
            # take the first value in the oneof
            out[k] = next(iter(v.values()))
        else:
            out[k] = v
    return out

# --- RabbitMQ Connection ---
def get_connection():
    while True:
        try:
            connection = pika.BlockingConnection(pika.ConnectionParameters(host=RABBITMQ_HOST))
            channel = connection.channel()
            channel.queue_declare(queue=LOG_QUEUE, durable=True)
            channel.queue_declare(queue=METRIC_QUEUE, durable=True)
            return connection, channel
        except Exception as e:
            logging.error(f"Failed to connect to RabbitMQ: {e}, retrying in 5s...")
            time.sleep(5)

def get_rabbitmq_channel():
    """Helper to quickly open connection+channel for publishing"""
    connection, channel = get_connection()
    return connection, channel

# --- Metric Transformer ---
def transform_metric(raw_body) -> list[dict] | None:
    """
    Convert OTLP metrics payload into flat JSON rows (BigQuery-friendly).
    Accepts bytes|str|dict.
    """
    msg = decode_body_to_obj(raw_body)
    if msg is None:
        logging.error("‚ö†Ô∏è Could not parse metric body as JSON")
        return None

    transformed = []

    # OTLP metrics are nested under resourceMetrics
    for rm in msg.get("resourceMetrics", []):
        resource_attrs = _attrs_list_to_dict(rm.get("resource", {}).get("attributes", []))

        for sm in rm.get("scopeMetrics", []):
            for metric in sm.get("metrics", []):
                metric_name = metric.get("name")

                # SUM
                if "sum" in metric and isinstance(metric["sum"], dict):
                    for dp in metric["sum"].get("dataPoints", []):
                        dp_attrs = _attrs_list_to_dict(dp.get("attributes", []))
                        row = {
                            "store_id": store_id,
                            "metric_name": metric_name,
                            "timestamp": dp.get("timeUnixNano"),
                            "value": dp.get("asDouble") if dp.get("asDouble") is not None else dp.get("asInt"),
                            "attributes": safe_json_str(dp_attrs),
                            "resource": safe_json_str(resource_attrs),
                        }
                        transformed.append(row)

                # GAUGE (optional support)
                if "gauge" in metric and isinstance(metric["gauge"], dict):
                    for dp in metric["gauge"].get("dataPoints", []):
                        dp_attrs = _attrs_list_to_dict(dp.get("attributes", []))
                        row = {
                            "store_id": store_id,
                            "metric_name": metric_name,
                            "timestamp": dp.get("timeUnixNano"),
                            "value": dp.get("asDouble") if dp.get("asDouble") is not None else dp.get("asInt"),
                            "attributes": safe_json_str(dp_attrs),
                            "resource": safe_json_str(resource_attrs),
                        }
                        transformed.append(row)

                # You can add histogram/exponentialHistogram here if needed.

    return transformed

# --- Downstream Sender ---

def send_downstream(body, is_metric=False):
    """
    POST to downstream. Metrics are transformed into list[dict].
    Logs are normalized into dict/list JSON.
    """
    if is_metric:
        payload = transform_metric(body)
        if not payload:
            logging.error("‚ùå Metric transform produced no rows; dropping.")
            return False
    else:
        obj = decode_body_to_obj(body)
        if obj is None:
            logging.error(f"‚ùå Log payload invalid JSON, repairing: {body[:200] if isinstance(body, (bytes,str)) else body}")
            try:
                import ast
                obj = ast.literal_eval(body.decode("utf-8") if isinstance(body, bytes) else str(body))
            except Exception as e:
                logging.error(f"‚ùå Could not repair log body: {e}")
                return False
        payload = obj

    # Ensure we always POST valid JSON
    try:
        payload_json = json.dumps(payload, ensure_ascii=False)
    except Exception as e:
        logging.error(f"‚ùå Payload not JSON serializable: {e}")
        return False

    while True:
        try:
            res = requests.post(DOWNSTREAM_URL, data=payload_json, headers={"Content-Type": "application/json"}, timeout=10)
            res.raise_for_status()
            logging.info(f"‚úÖ Sent to downstream ({'metrics' if is_metric else 'logs'}): {str(payload)[:300]}...")
            return True
        except Exception as e:
            logging.error(f"üåê Downstream error: {e}, retrying in 5s...")
            time.sleep(5)


# --- Consumer Callback ---
def callback(ch, method, properties, body, queue_type="logs"):
    # body is bytes from RabbitMQ
    preview = body[:200].decode("utf-8", errors="replace") if isinstance(body, (bytes, bytearray)) else str(body)[:200]
    logging.info(f"üì• Got message from {queue_type}: {preview}...")
    if queue_type == "metrics":
        ok = send_downstream(body, is_metric=True)
    else:
        ok = send_downstream(body, is_metric=False)

    if ok:
        ch.basic_ack(delivery_tag=method.delivery_tag)
    else:
        # drop to DLQ or discard; set requeue=True if you want retries at RabbitMQ layer
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)

# --- Consumer Worker ---
def start_consumer(queue_name, queue_type="logs"):
    while True:
        try:
            connection, channel = get_connection()
            channel.basic_qos(prefetch_count=1)
            channel.basic_consume(
                queue=queue_name,
                on_message_callback=lambda ch, method, props, body: callback(ch, method, props, body, queue_type)
            )

            logging.info(f"üöÄ RabbitMQ consumer started for {queue_type} queue: {queue_name}")
            channel.start_consuming()
        except Exception as e:
            logging.error(f"‚ùå Consumer for {queue_type} crashed: {e}, retrying in 5s...")
            time.sleep(5)

@app.post("/log")
async def log_message(request: Request):
    try:
        """Publish incoming JSON to RabbitMQ"""
        data = await request.json()

        timestamp = data.get("timestamp") or time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

        # Build enriched payload (keep as dict; we json.dumps only when publishing)
        payload = {
            "store_id": "store_123",
            "timestamp": timestamp,
            "app_info": data.get("app_info"),
            "message_id": data.get("message_id"),
            "event": data.get("event"),
            "event_value": data.get("event_value"),
            "insert_id": f"unique_message_id_{store_id}_{int(time.time())}"
        }

        connection, channel = get_rabbitmq_channel()
        channel.basic_publish(
            exchange="",
            routing_key=LOG_QUEUE,
            body=json.dumps(payload),
            properties=pika.BasicProperties(delivery_mode=2)  # persistent
        )
        logging.info(f"üì§ Published to RabbitMQ logs_queue: {payload}")
        connection.close()
        return {"status": "Message sent to RabbitMQ", "data": data}
    except Exception as e:
        logging.error(f"Error processing log: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =========================
# FASTAPI STARTUP EVENT
# =========================
@app.on_event("startup")
def startup_event():
    logging.info("‚ö° Launching RabbitMQ consumers in background threads...")
    threading.Thread(target=start_consumer, args=(LOG_QUEUE, "logs"), daemon=True).start()
    threading.Thread(target=start_consumer, args=(METRIC_QUEUE, "metrics"), daemon=True).start()

# =========================
# MAIN ENTRYPOINT
# =========================
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5000)

